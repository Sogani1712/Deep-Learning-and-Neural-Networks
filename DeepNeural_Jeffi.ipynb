{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris\n",
    "iris_dataset = load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['data', 'target', 'target_names', 'DESCR', 'feature_names', 'filename'])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris_dataset.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sepal length (cm)',\n",
       " 'sepal width (cm)',\n",
       " 'petal length (cm)',\n",
       " 'petal width (cm)']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris_dataset.feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal length (cm)</th>\n",
       "      <th>sepal width (cm)</th>\n",
       "      <th>petal length (cm)</th>\n",
       "      <th>petal width (cm)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)\n",
       "0                5.1               3.5                1.4               0.2\n",
       "1                4.9               3.0                1.4               0.2\n",
       "2                4.7               3.2                1.3               0.2\n",
       "3                4.6               3.1                1.5               0.2\n",
       "4                5.0               3.6                1.4               0.2"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris = pd.DataFrame(data = iris_dataset.data,columns = iris_dataset.feature_names)\n",
    "iris.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 150 entries, 0 to 149\n",
      "Data columns (total 4 columns):\n",
      "sepal length (cm)    150 non-null float64\n",
      "sepal width (cm)     150 non-null float64\n",
      "petal length (cm)    150 non-null float64\n",
      "petal width (cm)     150 non-null float64\n",
      "dtypes: float64(4)\n",
      "memory usage: 4.8 KB\n"
     ]
    }
   ],
   "source": [
    "iris.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'mean': 5.843333333333335,\n",
      " 'name': 'sepal length (cm)',\n",
      " 'sdev': 0.8280661279778629,\n",
      " 'var': 0.6856935123042505}\n",
      "{'mean': 3.057333333333334,\n",
      " 'name': 'sepal width (cm)',\n",
      " 'sdev': 0.435866284936698,\n",
      " 'var': 0.1899794183445188}\n",
      "{'mean': 3.7580000000000027,\n",
      " 'name': 'petal length (cm)',\n",
      " 'sdev': 1.7652982332594667,\n",
      " 'var': 3.1162778523489942}\n",
      "{'mean': 1.199333333333334,\n",
      " 'name': 'petal width (cm)',\n",
      " 'sdev': 0.7622376689603465,\n",
      " 'var': 0.5810062639821029}\n"
     ]
    }
   ],
   "source": [
    "# Strip non-numerics\n",
    "iris = iris.select_dtypes(include=['int', 'float'])\n",
    "\n",
    "headers = list(iris.columns.values)\n",
    "fields = []\n",
    "\n",
    "for field in headers:\n",
    "    fields.append({\n",
    "        'name' : field,\n",
    "        'mean': iris[field].mean(),\n",
    "        'var': iris[field].var(),\n",
    "        'sdev': iris[field].std()\n",
    "    })\n",
    "\n",
    "for field in fields:\n",
    "    pprint(field)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>name</th>\n",
       "      <th>sdev</th>\n",
       "      <th>var</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.843333</td>\n",
       "      <td>sepal length (cm)</td>\n",
       "      <td>0.828066</td>\n",
       "      <td>0.685694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.057333</td>\n",
       "      <td>sepal width (cm)</td>\n",
       "      <td>0.435866</td>\n",
       "      <td>0.189979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.758000</td>\n",
       "      <td>petal length (cm)</td>\n",
       "      <td>1.765298</td>\n",
       "      <td>3.116278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.199333</td>\n",
       "      <td>petal width (cm)</td>\n",
       "      <td>0.762238</td>\n",
       "      <td>0.581006</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       mean               name      sdev       var\n",
       "0  5.843333  sepal length (cm)  0.828066  0.685694\n",
       "1  3.057333   sepal width (cm)  0.435866  0.189979\n",
       "2  3.758000  petal length (cm)  1.765298  3.116278\n",
       "3  1.199333   petal width (cm)  0.762238  0.581006"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.set_option('display.max_columns', 0)\n",
    "pd.set_option('display.max_rows', 0)\n",
    "df2 = pd.DataFrame(fields)\n",
    "display(df2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outliers are values that are unusually high or low. Sometimes outliers are simply errors; \n",
    "### this is a result of observation error. Outliers can also be truly large or small values that may be difficult to address. \n",
    "### We typically consider outliers to be a value that is several standard deviations from the mean.\n",
    "### The following function can remove such values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = iris\n",
    "# Remove all rows where the specified column is +/- sd standard deviations\n",
    "def remove_outliers(df, name, sd):\n",
    "    drop_rows = df.index[(np.abs(df[name] - df[name].mean())\n",
    "                          >= (sd * df[name].std()))]\n",
    "    df.drop(drop_rows, axis=0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length before sepal width outliers dropped: 150\n",
      "Length after sepal width outliers dropped: 145\n"
     ]
    }
   ],
   "source": [
    "# Drop outliers in iris-dataset\n",
    "#where the sepal width is more than two standard deviations above or below the mean.\n",
    "import numpy as np\n",
    "print(\"Length before sepal width outliers dropped: {}\".format(len(df)))\n",
    "remove_outliers(df,'sepal width (cm)',2)\n",
    "print(\"Length after sepal width outliers dropped: {}\".format(len(df)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dropping Fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some fields are of no value to the neural network should be dropped.\n",
    "# The following code removes the name column from the MPG dataset.\n",
    "# df.drop('name', 1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Usually a good idea to shuffle\n",
    "# df = df.reindex(np.random.permutation(df.index)) \n",
    "# mask = np.random.rand(len(df)) < 0.8\n",
    "# trainDF = pd.DataFrame(df[mask])\n",
    "# # validationDF = pd.DataFrame(df[~mask])\n",
    "# print(f\"Training DF: {len(trainDF)}\")\n",
    "# print(f\"Validation DF: {len(validationDF)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "\n",
    "# path = \".\"  # put path of your directory \n",
    "\n",
    "# df = pd.read_csv(\n",
    "#     \"https://data.heatonresearch.com/data/t81-558/auto-mpg.csv\",\n",
    "#     na_values=['NA','?'])\n",
    "\n",
    "# filename_write = os.path.join(path, \"auto-mpg-shuffle.csv\")\n",
    "# df = df.reindex(np.random.permutation(df.index))\n",
    "# # Specify index = false to not write row numbers\n",
    "# df.to_csv(filename_write, index=False) \n",
    "# print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pickle will restore your index (helpful in stock data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoding Continuous Value\n",
    "\n",
    "### Always normalize(Get Z score which tells us how far(std dev) we are from our mean value.It fixes range "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mpg</th>\n",
       "      <th>cylinders</th>\n",
       "      <th>displacement</th>\n",
       "      <th>...</th>\n",
       "      <th>year</th>\n",
       "      <th>origin</th>\n",
       "      <th>name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.706439</td>\n",
       "      <td>8</td>\n",
       "      <td>307.0</td>\n",
       "      <td>...</td>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "      <td>chevrolet chevelle malibu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.090751</td>\n",
       "      <td>8</td>\n",
       "      <td>350.0</td>\n",
       "      <td>...</td>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "      <td>buick skylark 320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>396</th>\n",
       "      <td>0.574601</td>\n",
       "      <td>4</td>\n",
       "      <td>120.0</td>\n",
       "      <td>...</td>\n",
       "      <td>82</td>\n",
       "      <td>1</td>\n",
       "      <td>ford ranger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>397</th>\n",
       "      <td>0.958913</td>\n",
       "      <td>4</td>\n",
       "      <td>119.0</td>\n",
       "      <td>...</td>\n",
       "      <td>82</td>\n",
       "      <td>1</td>\n",
       "      <td>chevy s-10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>398 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          mpg  cylinders  displacement  ...  year  origin  \\\n",
       "0   -0.706439          8         307.0  ...    70       1   \n",
       "1   -1.090751          8         350.0  ...    70       1   \n",
       "..        ...        ...           ...  ...   ...     ...   \n",
       "396  0.574601          4         120.0  ...    82       1   \n",
       "397  0.958913          4         119.0  ...    82       1   \n",
       "\n",
       "                          name  \n",
       "0    chevrolet chevelle malibu  \n",
       "1            buick skylark 320  \n",
       "..                         ...  \n",
       "396                ford ranger  \n",
       "397                 chevy s-10  \n",
       "\n",
       "[398 rows x 9 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from scipy.stats import zscore\n",
    "\n",
    "df1= pd.read_csv(\n",
    "    \"https://data.heatonresearch.com/data/t81-558/auto-mpg.csv\",\n",
    "    na_values=['NA','?'])\n",
    "\n",
    "pd.set_option('display.max_columns', 7)\n",
    "pd.set_option('display.max_rows', 5)\n",
    "\n",
    "df1['mpg'] = zscore(df1['mpg'])\n",
    "display(df1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoding Continuous Value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>job</th>\n",
       "      <th>area</th>\n",
       "      <th>income</th>\n",
       "      <th>aspect</th>\n",
       "      <th>subscriptions</th>\n",
       "      <th>dist_healthy</th>\n",
       "      <th>save_rate</th>\n",
       "      <th>dist_unhealthy</th>\n",
       "      <th>age</th>\n",
       "      <th>pop_dense</th>\n",
       "      <th>retail_dense</th>\n",
       "      <th>crime</th>\n",
       "      <th>product</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>vv</td>\n",
       "      <td>c</td>\n",
       "      <td>50876.0</td>\n",
       "      <td>13.100000</td>\n",
       "      <td>1</td>\n",
       "      <td>9.017895</td>\n",
       "      <td>35</td>\n",
       "      <td>11.738935</td>\n",
       "      <td>49</td>\n",
       "      <td>0.885827</td>\n",
       "      <td>0.492126</td>\n",
       "      <td>0.071100</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>kd</td>\n",
       "      <td>c</td>\n",
       "      <td>60369.0</td>\n",
       "      <td>18.625000</td>\n",
       "      <td>2</td>\n",
       "      <td>7.766643</td>\n",
       "      <td>59</td>\n",
       "      <td>6.805396</td>\n",
       "      <td>51</td>\n",
       "      <td>0.874016</td>\n",
       "      <td>0.342520</td>\n",
       "      <td>0.400809</td>\n",
       "      <td>c</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>pe</td>\n",
       "      <td>c</td>\n",
       "      <td>55126.0</td>\n",
       "      <td>34.766667</td>\n",
       "      <td>1</td>\n",
       "      <td>3.632069</td>\n",
       "      <td>6</td>\n",
       "      <td>13.671772</td>\n",
       "      <td>44</td>\n",
       "      <td>0.944882</td>\n",
       "      <td>0.724409</td>\n",
       "      <td>0.207723</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>11</td>\n",
       "      <td>c</td>\n",
       "      <td>51690.0</td>\n",
       "      <td>15.808333</td>\n",
       "      <td>1</td>\n",
       "      <td>5.372942</td>\n",
       "      <td>16</td>\n",
       "      <td>4.333286</td>\n",
       "      <td>50</td>\n",
       "      <td>0.889764</td>\n",
       "      <td>0.444882</td>\n",
       "      <td>0.361216</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>kl</td>\n",
       "      <td>d</td>\n",
       "      <td>28347.0</td>\n",
       "      <td>40.941667</td>\n",
       "      <td>3</td>\n",
       "      <td>3.822477</td>\n",
       "      <td>20</td>\n",
       "      <td>5.967121</td>\n",
       "      <td>38</td>\n",
       "      <td>0.744094</td>\n",
       "      <td>0.661417</td>\n",
       "      <td>0.068033</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>e2</td>\n",
       "      <td>c</td>\n",
       "      <td>70854.0</td>\n",
       "      <td>40.400000</td>\n",
       "      <td>1</td>\n",
       "      <td>14.893343</td>\n",
       "      <td>87</td>\n",
       "      <td>20.340593</td>\n",
       "      <td>43</td>\n",
       "      <td>0.866142</td>\n",
       "      <td>0.673228</td>\n",
       "      <td>0.473581</td>\n",
       "      <td>d</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>kl</td>\n",
       "      <td>d</td>\n",
       "      <td>38726.0</td>\n",
       "      <td>30.975000</td>\n",
       "      <td>3</td>\n",
       "      <td>3.822477</td>\n",
       "      <td>33</td>\n",
       "      <td>9.480399</td>\n",
       "      <td>39</td>\n",
       "      <td>0.976378</td>\n",
       "      <td>0.874016</td>\n",
       "      <td>0.092151</td>\n",
       "      <td>f</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>nb</td>\n",
       "      <td>a</td>\n",
       "      <td>55162.0</td>\n",
       "      <td>26.966667</td>\n",
       "      <td>2</td>\n",
       "      <td>4.312097</td>\n",
       "      <td>17</td>\n",
       "      <td>29.219896</td>\n",
       "      <td>44</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.724409</td>\n",
       "      <td>0.162833</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>al</td>\n",
       "      <td>c</td>\n",
       "      <td>67311.0</td>\n",
       "      <td>32.383333</td>\n",
       "      <td>0</td>\n",
       "      <td>25.093772</td>\n",
       "      <td>169</td>\n",
       "      <td>10.927357</td>\n",
       "      <td>45</td>\n",
       "      <td>0.952756</td>\n",
       "      <td>0.681102</td>\n",
       "      <td>0.096333</td>\n",
       "      <td>c</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1991</th>\n",
       "      <td>1992</td>\n",
       "      <td>nb</td>\n",
       "      <td>a</td>\n",
       "      <td>64088.0</td>\n",
       "      <td>10.283333</td>\n",
       "      <td>2</td>\n",
       "      <td>11.520401</td>\n",
       "      <td>67</td>\n",
       "      <td>28.803428</td>\n",
       "      <td>48</td>\n",
       "      <td>0.799213</td>\n",
       "      <td>0.393701</td>\n",
       "      <td>0.453646</td>\n",
       "      <td>c</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1992</th>\n",
       "      <td>1993</td>\n",
       "      <td>ob</td>\n",
       "      <td>a</td>\n",
       "      <td>56791.0</td>\n",
       "      <td>6.491667</td>\n",
       "      <td>1</td>\n",
       "      <td>4.312097</td>\n",
       "      <td>9</td>\n",
       "      <td>29.700436</td>\n",
       "      <td>46</td>\n",
       "      <td>0.842520</td>\n",
       "      <td>0.535433</td>\n",
       "      <td>0.406246</td>\n",
       "      <td>c</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1993</th>\n",
       "      <td>1994</td>\n",
       "      <td>kl</td>\n",
       "      <td>d</td>\n",
       "      <td>35502.0</td>\n",
       "      <td>33.250000</td>\n",
       "      <td>0</td>\n",
       "      <td>2.816034</td>\n",
       "      <td>0</td>\n",
       "      <td>2.507236</td>\n",
       "      <td>45</td>\n",
       "      <td>0.948819</td>\n",
       "      <td>0.681102</td>\n",
       "      <td>0.105256</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1994</th>\n",
       "      <td>1995</td>\n",
       "      <td>qp</td>\n",
       "      <td>c</td>\n",
       "      <td>58704.0</td>\n",
       "      <td>40.833333</td>\n",
       "      <td>1</td>\n",
       "      <td>5.372942</td>\n",
       "      <td>25</td>\n",
       "      <td>7.125755</td>\n",
       "      <td>43</td>\n",
       "      <td>0.870079</td>\n",
       "      <td>0.657480</td>\n",
       "      <td>0.276872</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1995</th>\n",
       "      <td>1996</td>\n",
       "      <td>vv</td>\n",
       "      <td>c</td>\n",
       "      <td>51017.0</td>\n",
       "      <td>38.233333</td>\n",
       "      <td>1</td>\n",
       "      <td>5.454545</td>\n",
       "      <td>34</td>\n",
       "      <td>14.013489</td>\n",
       "      <td>41</td>\n",
       "      <td>0.881890</td>\n",
       "      <td>0.744094</td>\n",
       "      <td>0.104838</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996</th>\n",
       "      <td>1997</td>\n",
       "      <td>kl</td>\n",
       "      <td>d</td>\n",
       "      <td>26576.0</td>\n",
       "      <td>33.358333</td>\n",
       "      <td>2</td>\n",
       "      <td>3.632069</td>\n",
       "      <td>20</td>\n",
       "      <td>8.380497</td>\n",
       "      <td>38</td>\n",
       "      <td>0.944882</td>\n",
       "      <td>0.877953</td>\n",
       "      <td>0.063851</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997</th>\n",
       "      <td>1998</td>\n",
       "      <td>kl</td>\n",
       "      <td>d</td>\n",
       "      <td>28595.0</td>\n",
       "      <td>39.425000</td>\n",
       "      <td>3</td>\n",
       "      <td>7.168218</td>\n",
       "      <td>99</td>\n",
       "      <td>4.626950</td>\n",
       "      <td>36</td>\n",
       "      <td>0.759843</td>\n",
       "      <td>0.744094</td>\n",
       "      <td>0.098703</td>\n",
       "      <td>f</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998</th>\n",
       "      <td>1999</td>\n",
       "      <td>qp</td>\n",
       "      <td>c</td>\n",
       "      <td>67949.0</td>\n",
       "      <td>5.733333</td>\n",
       "      <td>0</td>\n",
       "      <td>8.936292</td>\n",
       "      <td>26</td>\n",
       "      <td>3.281439</td>\n",
       "      <td>46</td>\n",
       "      <td>0.909449</td>\n",
       "      <td>0.598425</td>\n",
       "      <td>0.117803</td>\n",
       "      <td>c</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999</th>\n",
       "      <td>2000</td>\n",
       "      <td>pe</td>\n",
       "      <td>c</td>\n",
       "      <td>61467.0</td>\n",
       "      <td>16.891667</td>\n",
       "      <td>0</td>\n",
       "      <td>4.312097</td>\n",
       "      <td>8</td>\n",
       "      <td>9.405648</td>\n",
       "      <td>48</td>\n",
       "      <td>0.925197</td>\n",
       "      <td>0.539370</td>\n",
       "      <td>0.451973</td>\n",
       "      <td>c</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2000 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id job area   income  ...  pop_dense  retail_dense     crime  product\n",
       "0        1  vv    c  50876.0  ...   0.885827      0.492126  0.071100        b\n",
       "1        2  kd    c  60369.0  ...   0.874016      0.342520  0.400809        c\n",
       "2        3  pe    c  55126.0  ...   0.944882      0.724409  0.207723        b\n",
       "3        4  11    c  51690.0  ...   0.889764      0.444882  0.361216        b\n",
       "4        5  kl    d  28347.0  ...   0.744094      0.661417  0.068033        a\n",
       "5        6  e2    c  70854.0  ...   0.866142      0.673228  0.473581        d\n",
       "6        7  kl    d  38726.0  ...   0.976378      0.874016  0.092151        f\n",
       "7        8  nb    a  55162.0  ...   1.000000      0.724409  0.162833        b\n",
       "8        9  al    c  67311.0  ...   0.952756      0.681102  0.096333        c\n",
       "...    ...  ..  ...      ...  ...        ...           ...       ...      ...\n",
       "1991  1992  nb    a  64088.0  ...   0.799213      0.393701  0.453646        c\n",
       "1992  1993  ob    a  56791.0  ...   0.842520      0.535433  0.406246        c\n",
       "1993  1994  kl    d  35502.0  ...   0.948819      0.681102  0.105256        a\n",
       "1994  1995  qp    c  58704.0  ...   0.870079      0.657480  0.276872        b\n",
       "1995  1996  vv    c  51017.0  ...   0.881890      0.744094  0.104838        b\n",
       "1996  1997  kl    d  26576.0  ...   0.944882      0.877953  0.063851        a\n",
       "1997  1998  kl    d  28595.0  ...   0.759843      0.744094  0.098703        f\n",
       "1998  1999  qp    c  67949.0  ...   0.909449      0.598425  0.117803        c\n",
       "1999  2000  pe    c  61467.0  ...   0.925197      0.539370  0.451973        c\n",
       "\n",
       "[2000 rows x 14 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\n",
    "    \"https://data.heatonresearch.com/data/t81-558/jh-simple-dataset.csv\",\n",
    "    na_values=['NA','?'])\n",
    "\n",
    "pd.set_option('display.max_columns', 0)\n",
    "pd.set_option('display.max_rows', 0)\n",
    "\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['c', 'd', 'a', 'b']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "areas = list(df['area'].unique())\n",
    "areas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(areas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>area_a</th>\n",
       "      <th>area_b</th>\n",
       "      <th>area_c</th>\n",
       "      <th>area_d</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1991</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1992</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1993</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1994</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1995</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2000 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      area_a  area_b  area_c  area_d\n",
       "0          0       0       1       0\n",
       "1          0       0       1       0\n",
       "2          0       0       1       0\n",
       "3          0       0       1       0\n",
       "4          0       0       0       1\n",
       "5          0       0       1       0\n",
       "6          0       0       0       1\n",
       "7          1       0       0       0\n",
       "8          0       0       1       0\n",
       "...      ...     ...     ...     ...\n",
       "1991       1       0       0       0\n",
       "1992       1       0       0       0\n",
       "1993       0       0       0       1\n",
       "1994       0       0       1       0\n",
       "1995       0       0       1       0\n",
       "1996       0       0       0       1\n",
       "1997       0       0       0       1\n",
       "1998       0       0       1       0\n",
       "1999       0       0       1       0\n",
       "\n",
       "[2000 rows x 4 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummies = pd.get_dummies(df['area'],prefix='area')\n",
    "dummies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>job</th>\n",
       "      <th>income</th>\n",
       "      <th>aspect</th>\n",
       "      <th>subscriptions</th>\n",
       "      <th>dist_healthy</th>\n",
       "      <th>save_rate</th>\n",
       "      <th>dist_unhealthy</th>\n",
       "      <th>age</th>\n",
       "      <th>pop_dense</th>\n",
       "      <th>retail_dense</th>\n",
       "      <th>crime</th>\n",
       "      <th>product</th>\n",
       "      <th>area_a</th>\n",
       "      <th>area_b</th>\n",
       "      <th>area_c</th>\n",
       "      <th>area_d</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>vv</td>\n",
       "      <td>50876.0</td>\n",
       "      <td>13.100000</td>\n",
       "      <td>1</td>\n",
       "      <td>9.017895</td>\n",
       "      <td>35</td>\n",
       "      <td>11.738935</td>\n",
       "      <td>49</td>\n",
       "      <td>0.885827</td>\n",
       "      <td>0.492126</td>\n",
       "      <td>0.071100</td>\n",
       "      <td>b</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>kd</td>\n",
       "      <td>60369.0</td>\n",
       "      <td>18.625000</td>\n",
       "      <td>2</td>\n",
       "      <td>7.766643</td>\n",
       "      <td>59</td>\n",
       "      <td>6.805396</td>\n",
       "      <td>51</td>\n",
       "      <td>0.874016</td>\n",
       "      <td>0.342520</td>\n",
       "      <td>0.400809</td>\n",
       "      <td>c</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>pe</td>\n",
       "      <td>55126.0</td>\n",
       "      <td>34.766667</td>\n",
       "      <td>1</td>\n",
       "      <td>3.632069</td>\n",
       "      <td>6</td>\n",
       "      <td>13.671772</td>\n",
       "      <td>44</td>\n",
       "      <td>0.944882</td>\n",
       "      <td>0.724409</td>\n",
       "      <td>0.207723</td>\n",
       "      <td>b</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>11</td>\n",
       "      <td>51690.0</td>\n",
       "      <td>15.808333</td>\n",
       "      <td>1</td>\n",
       "      <td>5.372942</td>\n",
       "      <td>16</td>\n",
       "      <td>4.333286</td>\n",
       "      <td>50</td>\n",
       "      <td>0.889764</td>\n",
       "      <td>0.444882</td>\n",
       "      <td>0.361216</td>\n",
       "      <td>b</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>kl</td>\n",
       "      <td>28347.0</td>\n",
       "      <td>40.941667</td>\n",
       "      <td>3</td>\n",
       "      <td>3.822477</td>\n",
       "      <td>20</td>\n",
       "      <td>5.967121</td>\n",
       "      <td>38</td>\n",
       "      <td>0.744094</td>\n",
       "      <td>0.661417</td>\n",
       "      <td>0.068033</td>\n",
       "      <td>a</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>e2</td>\n",
       "      <td>70854.0</td>\n",
       "      <td>40.400000</td>\n",
       "      <td>1</td>\n",
       "      <td>14.893343</td>\n",
       "      <td>87</td>\n",
       "      <td>20.340593</td>\n",
       "      <td>43</td>\n",
       "      <td>0.866142</td>\n",
       "      <td>0.673228</td>\n",
       "      <td>0.473581</td>\n",
       "      <td>d</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>kl</td>\n",
       "      <td>38726.0</td>\n",
       "      <td>30.975000</td>\n",
       "      <td>3</td>\n",
       "      <td>3.822477</td>\n",
       "      <td>33</td>\n",
       "      <td>9.480399</td>\n",
       "      <td>39</td>\n",
       "      <td>0.976378</td>\n",
       "      <td>0.874016</td>\n",
       "      <td>0.092151</td>\n",
       "      <td>f</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>nb</td>\n",
       "      <td>55162.0</td>\n",
       "      <td>26.966667</td>\n",
       "      <td>2</td>\n",
       "      <td>4.312097</td>\n",
       "      <td>17</td>\n",
       "      <td>29.219896</td>\n",
       "      <td>44</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.724409</td>\n",
       "      <td>0.162833</td>\n",
       "      <td>b</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>al</td>\n",
       "      <td>67311.0</td>\n",
       "      <td>32.383333</td>\n",
       "      <td>0</td>\n",
       "      <td>25.093772</td>\n",
       "      <td>169</td>\n",
       "      <td>10.927357</td>\n",
       "      <td>45</td>\n",
       "      <td>0.952756</td>\n",
       "      <td>0.681102</td>\n",
       "      <td>0.096333</td>\n",
       "      <td>c</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1991</th>\n",
       "      <td>1992</td>\n",
       "      <td>nb</td>\n",
       "      <td>64088.0</td>\n",
       "      <td>10.283333</td>\n",
       "      <td>2</td>\n",
       "      <td>11.520401</td>\n",
       "      <td>67</td>\n",
       "      <td>28.803428</td>\n",
       "      <td>48</td>\n",
       "      <td>0.799213</td>\n",
       "      <td>0.393701</td>\n",
       "      <td>0.453646</td>\n",
       "      <td>c</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1992</th>\n",
       "      <td>1993</td>\n",
       "      <td>ob</td>\n",
       "      <td>56791.0</td>\n",
       "      <td>6.491667</td>\n",
       "      <td>1</td>\n",
       "      <td>4.312097</td>\n",
       "      <td>9</td>\n",
       "      <td>29.700436</td>\n",
       "      <td>46</td>\n",
       "      <td>0.842520</td>\n",
       "      <td>0.535433</td>\n",
       "      <td>0.406246</td>\n",
       "      <td>c</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1993</th>\n",
       "      <td>1994</td>\n",
       "      <td>kl</td>\n",
       "      <td>35502.0</td>\n",
       "      <td>33.250000</td>\n",
       "      <td>0</td>\n",
       "      <td>2.816034</td>\n",
       "      <td>0</td>\n",
       "      <td>2.507236</td>\n",
       "      <td>45</td>\n",
       "      <td>0.948819</td>\n",
       "      <td>0.681102</td>\n",
       "      <td>0.105256</td>\n",
       "      <td>a</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1994</th>\n",
       "      <td>1995</td>\n",
       "      <td>qp</td>\n",
       "      <td>58704.0</td>\n",
       "      <td>40.833333</td>\n",
       "      <td>1</td>\n",
       "      <td>5.372942</td>\n",
       "      <td>25</td>\n",
       "      <td>7.125755</td>\n",
       "      <td>43</td>\n",
       "      <td>0.870079</td>\n",
       "      <td>0.657480</td>\n",
       "      <td>0.276872</td>\n",
       "      <td>b</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1995</th>\n",
       "      <td>1996</td>\n",
       "      <td>vv</td>\n",
       "      <td>51017.0</td>\n",
       "      <td>38.233333</td>\n",
       "      <td>1</td>\n",
       "      <td>5.454545</td>\n",
       "      <td>34</td>\n",
       "      <td>14.013489</td>\n",
       "      <td>41</td>\n",
       "      <td>0.881890</td>\n",
       "      <td>0.744094</td>\n",
       "      <td>0.104838</td>\n",
       "      <td>b</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996</th>\n",
       "      <td>1997</td>\n",
       "      <td>kl</td>\n",
       "      <td>26576.0</td>\n",
       "      <td>33.358333</td>\n",
       "      <td>2</td>\n",
       "      <td>3.632069</td>\n",
       "      <td>20</td>\n",
       "      <td>8.380497</td>\n",
       "      <td>38</td>\n",
       "      <td>0.944882</td>\n",
       "      <td>0.877953</td>\n",
       "      <td>0.063851</td>\n",
       "      <td>a</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997</th>\n",
       "      <td>1998</td>\n",
       "      <td>kl</td>\n",
       "      <td>28595.0</td>\n",
       "      <td>39.425000</td>\n",
       "      <td>3</td>\n",
       "      <td>7.168218</td>\n",
       "      <td>99</td>\n",
       "      <td>4.626950</td>\n",
       "      <td>36</td>\n",
       "      <td>0.759843</td>\n",
       "      <td>0.744094</td>\n",
       "      <td>0.098703</td>\n",
       "      <td>f</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998</th>\n",
       "      <td>1999</td>\n",
       "      <td>qp</td>\n",
       "      <td>67949.0</td>\n",
       "      <td>5.733333</td>\n",
       "      <td>0</td>\n",
       "      <td>8.936292</td>\n",
       "      <td>26</td>\n",
       "      <td>3.281439</td>\n",
       "      <td>46</td>\n",
       "      <td>0.909449</td>\n",
       "      <td>0.598425</td>\n",
       "      <td>0.117803</td>\n",
       "      <td>c</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999</th>\n",
       "      <td>2000</td>\n",
       "      <td>pe</td>\n",
       "      <td>61467.0</td>\n",
       "      <td>16.891667</td>\n",
       "      <td>0</td>\n",
       "      <td>4.312097</td>\n",
       "      <td>8</td>\n",
       "      <td>9.405648</td>\n",
       "      <td>48</td>\n",
       "      <td>0.925197</td>\n",
       "      <td>0.539370</td>\n",
       "      <td>0.451973</td>\n",
       "      <td>c</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2000 rows × 17 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id job   income     aspect  ...  area_a  area_b  area_c  area_d\n",
       "0        1  vv  50876.0  13.100000  ...       0       0       1       0\n",
       "1        2  kd  60369.0  18.625000  ...       0       0       1       0\n",
       "2        3  pe  55126.0  34.766667  ...       0       0       1       0\n",
       "3        4  11  51690.0  15.808333  ...       0       0       1       0\n",
       "4        5  kl  28347.0  40.941667  ...       0       0       0       1\n",
       "5        6  e2  70854.0  40.400000  ...       0       0       1       0\n",
       "6        7  kl  38726.0  30.975000  ...       0       0       0       1\n",
       "7        8  nb  55162.0  26.966667  ...       1       0       0       0\n",
       "8        9  al  67311.0  32.383333  ...       0       0       1       0\n",
       "...    ...  ..      ...        ...  ...     ...     ...     ...     ...\n",
       "1991  1992  nb  64088.0  10.283333  ...       1       0       0       0\n",
       "1992  1993  ob  56791.0   6.491667  ...       1       0       0       0\n",
       "1993  1994  kl  35502.0  33.250000  ...       0       0       0       1\n",
       "1994  1995  qp  58704.0  40.833333  ...       0       0       1       0\n",
       "1995  1996  vv  51017.0  38.233333  ...       0       0       1       0\n",
       "1996  1997  kl  26576.0  33.358333  ...       0       0       0       1\n",
       "1997  1998  kl  28595.0  39.425000  ...       0       0       0       1\n",
       "1998  1999  qp  67949.0   5.733333  ...       0       0       1       0\n",
       "1999  2000  pe  61467.0  16.891667  ...       0       0       1       0\n",
       "\n",
       "[2000 rows x 17 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.concat([df,dummies],axis=1)\n",
    "df.drop('area', axis=1, inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Target Encoding for Categoricals\n",
    "###### Target encoding can sometimes increase the predictive power of a machine learning model. However, it also dramatically increases the risk of overfitting. Because of this risk, you must take care if you are using this method.   \n",
    "\n",
    "# Target encoding is a popular technique for Kaggle competitions.\n",
    "\n",
    "###### Generally, target encoding can only be used on a categorical feature when the output of the machine learning model is numeric (regression).\n",
    "\n",
    "###### The concept of target encoding is straightforward. For each category, we calculate the average target value for that category. Then to encode, we substitute the percent that corresponds to the category that the categorical value has. Unlike dummy variables, where you have a column for each category, with target encoding, the program only needs a single column.\n",
    "###### In this way, target coding is more efficient than dummy variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Apple', 'Pear', 'Orange', 'Pine apple']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def process_string(str):\n",
    "    t = str.strip()\n",
    "    return t[0].upper()+t[1:]\n",
    "l = ['   apple  ', 'pear ', 'orange', 'pine apple  ']\n",
    "list(map(process_string, l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Apple', 'Pear', 'Orange', 'Pine apple']\n"
     ]
    }
   ],
   "source": [
    "list1 = ['   apple  ', 'pear ', 'orange', 'pine apple  ']\n",
    "l2 = [process_string(x) for x in list1]\n",
    "print(l2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10, 20]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def greater_than_five(x):\n",
    "    return x>5\n",
    "\n",
    "l = [ 1, 10, 20, 3, -2, 0]\n",
    "l2 = list(filter(greater_than_five, l))\n",
    "print(l2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10, 20]\n"
     ]
    }
   ],
   "source": [
    "l = [ 1, 10, 20, 3, -2, 0]\n",
    "l2 = list(filter(lambda x: x>5, l))\n",
    "print(l2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cont_9</th>\n",
       "      <th>cat_0</th>\n",
       "      <th>cat_1</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11.505457</td>\n",
       "      <td>dog</td>\n",
       "      <td>wolf</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>60.906654</td>\n",
       "      <td>dog</td>\n",
       "      <td>wolf</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13.339096</td>\n",
       "      <td>dog</td>\n",
       "      <td>wolf</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>24.058962</td>\n",
       "      <td>dog</td>\n",
       "      <td>wolf</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>32.713906</td>\n",
       "      <td>dog</td>\n",
       "      <td>wolf</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>85.913749</td>\n",
       "      <td>cat</td>\n",
       "      <td>wolf</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>66.609021</td>\n",
       "      <td>cat</td>\n",
       "      <td>wolf</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>54.116221</td>\n",
       "      <td>cat</td>\n",
       "      <td>wolf</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2.901382</td>\n",
       "      <td>cat</td>\n",
       "      <td>wolf</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>73.374830</td>\n",
       "      <td>cat</td>\n",
       "      <td>tiger</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      cont_9 cat_0  cat_1  target\n",
       "0  11.505457   dog   wolf       1\n",
       "1  60.906654   dog   wolf       0\n",
       "2  13.339096   dog   wolf       1\n",
       "3  24.058962   dog   wolf       1\n",
       "4  32.713906   dog   wolf       1\n",
       "5  85.913749   cat   wolf       1\n",
       "6  66.609021   cat   wolf       0\n",
       "7  54.116221   cat   wolf       0\n",
       "8   2.901382   cat   wolf       0\n",
       "9  73.374830   cat  tiger       0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "np.random.seed(43)\n",
    "dog_tiger_data= pd.DataFrame({\n",
    "    'cont_9': np.random.rand(10)*100,\n",
    "    'cat_0': ['dog'] * 5 + ['cat'] * 5,\n",
    "    'cat_1': ['wolf'] * 9 + ['tiger'] * 1,\n",
    "    'target': [1, 0, 1, 1, 1, 1, 0, 0, 0, 0]\n",
    "})\n",
    "\n",
    "pd.set_option('display.max_columns', 0)\n",
    "pd.set_option('display.max_rows', 0)\n",
    "display(dog_tiger_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 0, 1, 1, 1, 1, 0, 0, 0, 0]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Here we can see that if we apply direct mean value for each of the category \n",
    "# we will get error for cat_1 becuz tiger is 1 and will give us wrong results.\n",
    "dog_tiger_data['target'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cat_0\n",
       "cat    0.2\n",
       "dog    0.8\n",
       "Name: target, dtype: float64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "means0 = dog_tiger_data.groupby('cat_0')['target'].mean()\n",
    "means0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cat': 0.2, 'dog': 0.8}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mea_n = dog_tiger_data.groupby('cat_0')['target'].mean().to_dict()\n",
    "mea_n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Smoothening the mean for target encoding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_smooth_mean(df1, df2, cat_name, weight, target):\n",
    "    # Compute the global mean\n",
    "    mean = df[ target ].mean()\n",
    "\n",
    "    # Compute the number of values and the mean of each group\n",
    "    agg = df.groupby(cat_name)[ target ].agg(['count', 'mean'])\n",
    "    counts = agg['count']\n",
    "    means = agg['mean']\n",
    "\n",
    "    # Compute the \"smoothed\" means\n",
    "    smooth = (counts * means + weight * mean) / (counts + weight)\n",
    "\n",
    "    # Replace each value by the according smoothed mean\n",
    "    if df2 is None:\n",
    "        return df1[cat_name].map(smooth)\n",
    "    else:\n",
    "        return df1[cat_name].map(smooth),df2[cat_name].map(smooth.to_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'target'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   2656\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2657\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2658\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'target'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-28-ae113b7fb5eb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mWEIGHT\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m df['cat_0_enc'] = calc_smooth_mean(df1=dog_tiger_data, df2=None, \n\u001b[1;32m----> 3\u001b[1;33m     cat_name='cat_0',target='target' ,weight=WEIGHT)\n\u001b[0m\u001b[0;32m      4\u001b[0m df['cat_1_enc'] = calc_smooth_mean(df1=dog_tiger_data, df2=None, \n\u001b[0;32m      5\u001b[0m     cat_name='cat_1',target='target' ,weight=WEIGHT)\n",
      "\u001b[1;32m<ipython-input-27-043636c474ba>\u001b[0m in \u001b[0;36mcalc_smooth_mean\u001b[1;34m(df1, df2, cat_name, weight, target)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mcalc_smooth_mean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdf2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcat_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[1;31m# Compute the global mean\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mmean\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m \u001b[0mtarget\u001b[0m \u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;31m# Compute the number of values and the mean of each group\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   2925\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2926\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2927\u001b[1;33m             \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2928\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2929\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   2657\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2658\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2659\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_cast_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2660\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtolerance\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2661\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'target'"
     ]
    }
   ],
   "source": [
    "WEIGHT = 5\n",
    "df['cat_0_enc'] = calc_smooth_mean(df1=dog_tiger_data, df2=None, \n",
    "    cat_name='cat_0',target='target' ,weight=WEIGHT)\n",
    "df['cat_1_enc'] = calc_smooth_mean(df1=dog_tiger_data, df2=None, \n",
    "    cat_name='cat_1',target='target' ,weight=WEIGHT)\n",
    "\n",
    "pd.set_option('display.max_columns', 0)\n",
    "pd.set_option('display.max_rows', 0)\n",
    "\n",
    "display(dog_tiger_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shuffling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for same index\n",
    "# np.random.seed(42) # Uncomment this line to get the same shuffle each time\n",
    "df = df.reindex(np.random.permutation(df.index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resetting the index values\n",
    "df.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sorting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sort_values(by='subscriptions', ascending= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = df.groupby('subscriptions')['age'].count().to_dict()\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apply And MAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the map\n",
    "df1['origin_name'] = df1['origin'].map({1: 'North America', 2: 'Europe', 3: 'Asia'}) \n",
    "\n",
    "# Shuffle the data, so that we hopefully see\n",
    "# more regions.\n",
    "df1 = df1.reindex(np.random.permutation(df1.index)) \n",
    "\n",
    "# Efficieny using apply \n",
    "efficiency = df1.apply(lambda x:x['displacement']/x['horsepower'],axis = 1)\n",
    "df1['efficiency'] = efficiency\n",
    "\n",
    "# Display\n",
    "pd.set_option('display.max_columns', 0)\n",
    "pd.set_option('display.max_rows', 10)\n",
    "display(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# Agri_data =pd.read_csv('https://www.irs.gov/pub/irs-soi/16zpallagi.csv')\n",
    "# Agri_data=Agri_data.loc[(Agri_data['zipcode']!=0) & (Agri_data['zipcode']!=99999),\n",
    "#           ['STATE','zipcode','agi_stub','N1']]\n",
    "# Agri_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "medians = {1:12500,2:37500,3:62500,4:87500,5:112500,6:212500}\n",
    "Agri_data['agi_stub']=df.agi_stub.map(medians)\n",
    "\n",
    "groups = df.groupby(by='zipcode')\n",
    "# Apply on groups\n",
    "Agri_data= pd.DataFrame(groups.apply(    \n",
    "    lambda x:sum(x['N1']*x['agi_stub'])/sum(x['N1']))) \\\n",
    "    .reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#how to insert a new column in dataframe using values of other columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.insert(1, 'weight_kg', (df['weight'] * 0.45359237).astype(int))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Great Circle Distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train_test_split for time-series data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Are they any special considerations when creating train/test splits for time series? If so, what and why?\n",
    "    \n",
    "Since our model is meant to predict events in the future, we must also validate the model on events in the future.\n",
    "If the data is mixed up between the training and test sets,then future data will leak in to the model and our validation results will overestimate the performance on new data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# whenn to check unique elements in a column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How to remove missing observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_id = max(df[df['obs_num'] == 0].index.tolist())+1  # Find the last zero and move one beyond\n",
    "print(start_id)\n",
    "df = df[start_id:] # Trim the rows that have missing observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dff = data_1.copy()\n",
    "pd.unique(dff.targetcolumn)\n",
    "dff.groupby('targetcolumn')['anothercolumn'].count()\n",
    "# Drop live(one of the value in state column) projects(here target column is 'state')\n",
    "dff = dff.query('state != \"live\"')\n",
    "\n",
    "# Add outcome column, \"successful\" == 1, others are 0\n",
    "dff= dff.assign(outcome=(dff['state'] == 'successful').astype(int))\n",
    "\n",
    "#Converting timestamps\n",
    "# I convert the launched feature(contains dates which are parsed from strings) into categorical features we can use in a model. \n",
    "# Since I loaded in the columns as timestamp data, \n",
    "# I access date and time values through the .dt attribute on the timestamp column.\n",
    "dff['day'] = dff['column name containing timestamp as 2017-11-06 15:13:23'].dt.day.astype('uint8')\n",
    "dff = dff.assign(hour=dff.launched.dt.hour.astype('uint8'),\n",
    "               day=dff.launched.dt.day,\n",
    "               month=dff.launched.dt.month,\n",
    "               year=dff.launched.dt.year)\n",
    "\n",
    "#Prepping categorical variables\n",
    "Now for the categorical variables -- I'll need to convert them into integers so our model can use the data. \n",
    "For this I'll use scikit-learn's LabelEncoder. \n",
    "This assigns an integer to each value of the categorical feature and replaces those values with the integers\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "cat_features = [col for col in dff.columns ifdff[col].dtype == \"object\"]\n",
    "encoder = LabelEncoder()\n",
    "\n",
    "# from sklearn.preprocessing import LabelEncoder\n",
    "# cat_features = [col for col in dff.columns ifdff[col].dtype == \"object\"]\n",
    "# encoder = preprocessing.LabelEncoder()\n",
    "\n",
    "# # Create new columns in clicks using preprocessing.LabelEncoder()\n",
    "\n",
    "# for feature in cat_features:\n",
    "#     dff[feature + '_labels'] = encoder.fit_transform(clicks[feature])\n",
    "\n",
    "# Apply the label encoder to each column\n",
    "encoded =dff[cat_features].apply(encoder.fit_transform)\n",
    "encoded.head(10)\n",
    "\n",
    "I'll collect all the features we'll use in a new dataframe and use that to train a model.\n",
    "data = ks[['goal', 'hour', 'day', 'month', 'year', 'outcome']].join(encoded)\n",
    "\n",
    "\n",
    "# For Training(80%),validation(10%) and testing(10%) of a  model\n",
    "valid_fraction = 0.1\n",
    "valid_size = int(len(data) * valid_fraction)\n",
    "\n",
    "train = data[:-2 * valid_size]\n",
    "valid = data[-2 * valid_size:-valid_size]\n",
    "test = data[-valid_size:]\n",
    "for each in [train, valid, test]:\n",
    "    print(f\"Outcome fraction = {each.outcome.mean():.4f}\")\n",
    "    \n",
    "# A good way to do this automatically is with sklearn.model_selection.StratifiedShuffleSplit\n",
    "#-------------------Training LightGBM Model-------------------------#\n",
    "\n",
    "import lightgbm as lgb\n",
    "\n",
    "feature_cols = train.columns.drop('outcome')\n",
    "\n",
    "dtrain = lgb.Dataset(train[feature_cols], label=train['outcome'])\n",
    "dvalid = lgb.Dataset(valid[feature_cols], label=valid['outcome'])\n",
    "\n",
    "param = {'num_leaves': 64, 'objective': 'binary'}\n",
    "param['metric'] = 'auc'\n",
    "num_round = 1000\n",
    "bst = lgb.train(param, dtrain, num_round, valid_sets=[dvalid], early_stopping_rounds=10, verbose_eval=False)\n",
    "from sklearn import metrics\n",
    "ypred = bst.predict(test[feature_cols])\n",
    "score = metrics.roc_auc_score(test['outcome'], ypred)\n",
    "\n",
    "print(f\"Test AUC score: {score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Count Encoding for categorical values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Count encoding replaces each categorical value with the number of times it appears in the dataset. \n",
    "For example, if the value \"GB\" occured 10 times in the country feature, then each \"GB\" would be replaced with the number 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Should write a function such as for splitting so that we will be able to repeat the process again\n",
    "def get_data_splits(dataframe, valid_fraction=0.1):\n",
    "    valid_fraction = 0.1\n",
    "    valid_size = int(len(dataframe) * valid_fraction)\n",
    "\n",
    "    train = dataframe[:-valid_size * 2]\n",
    "    # valid size == test size, last two sections of the data\n",
    "    valid = dataframe[-valid_size * 2:-valid_size]\n",
    "    test = dataframe[-valid_size:]\n",
    "    \n",
    "    return train, valid, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import category_encoders as ce\n",
    "\n",
    "cat_features = ['ip', 'app', 'device', 'os', 'channel']\n",
    "train, valid, test = get_data_splits(clicks)\n",
    " # Create the count encoder\n",
    "count_enc = ce.CountEncoder(cols=cat_features)\n",
    "# Learn encoding from the training set\n",
    "count_enc.fit(train[cat_features])\n",
    "\n",
    "    # Apply encoding to the train and validation sets\n",
    "train_encoded = train.join(count_enc.transform(train[cat_features]).add_suffix('_count'))\n",
    "valid_encoded = valid.join(count_enc.transform(valid[cat_features]).add_suffix('_count'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Target Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Target encoding replaces a categorical value with the average value of the target for that value of the feature.\n",
    "For example, given the country value \"CA\", you'd calculate the average outcome for all the rows with country == 'CA', around 0.28. \n",
    "This is often blended with the target probability over the entire dataset to reduce the variance of values with few occurences.\n",
    "# Try to check unique levels of category in a column\n",
    "This technique uses the targets to create new features. So including the validation or test data in the target encodings would be a form of target leakage.\n",
    "Instead, you should learn the target encodings from the training dataset only and apply it to the other datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import category_encoders as ce\n",
    "cat_features = ['category', 'currency', 'country']\n",
    "\n",
    "# Create the encoder itself\n",
    "target_enc = ce.TargetEncoder(cols=cat_features)\n",
    "\n",
    "train, valid, _ = get_data_splits(data)\n",
    "\n",
    "# Fit the encoder using the categorical features and target\n",
    "target_enc.fit(train[cat_features], train['outcome'])\n",
    "\n",
    "# Transform the features, rename the columns with _target suffix, and join to dataframe\n",
    "train = train.join(target_enc.transform(train[cat_features]).add_suffix('_target'))\n",
    "valid = valid.join(target_enc.transform(valid[cat_features]).add_suffix('_target'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CatBoost Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Similiar to target Encoding but....\n",
    "with CatBoost, for each row, the target probability is calculated only from the rows before it.\n",
    "better than target Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace this only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#target_enc = ce.CatBoostEncoder(cols=cat_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded = cb_enc.transform(clicks[cat_features])\n",
    "for col in encoded:\n",
    "    clicks.insert(len(clicks.columns), col + '_cb', encoded[col])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Method-1 Interaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Build interaction features from all pairs of categorical features.\n",
    "example, if one record has the country \"CA\" and category \"Music\", you can create a new value \"CA_Music\".\n",
    "Then, label encode the interaction feature and add it to our data.\n",
    "\n",
    "import itertools\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "interactions = pd.DataFrame(index=dff.index)\n",
    "for col1, col2 in itertools.combinations(cat_features, 2):\n",
    "    new_col_name = '_'.join([col1, col2])\n",
    "    # Convert to strings and combine\n",
    "    new_values = dff[col1].map(str) + \"_\" + dff[col2].map(str)\n",
    "    label_enc = LabelEncoder()\n",
    "    interactions[new_col_name] = label_enc.fit_transform(new_values)\n",
    "data = data.join(interactions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# applying to count no of projects in last week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "I'll create the series, using ks.launched as the index and ks.index as the values, then sort the times. \n",
    "Using a time series as the index allows us to define the rolling window size in terms of hours, days, weeks, etc.\n",
    "\n",
    "# First, create a Series with a timestamp index\n",
    "launched = pd.Series(ks.index, index=ks.launched, name=\"count_7_days\").sort_index()\n",
    "count_7_days = launched.rolling('7d').count() - 1\n",
    "# subtracting 1 to remove cuuent project\n",
    "\n",
    "we need to adjust the index so we can join it with the other training data.\n",
    "count_7_days.index = launched.values\n",
    "count_7_days = count_7_days.reindex(ks.index)\n",
    "data.join(count_7_days)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Time since the last project in the same category\n",
    "Do projects in the same category compete for donors?\n",
    "If you're trying to fund a video game and another game project was just launched, you might not get as much money. \n",
    "We can capture this by calculating the time since the last launch project in the same category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_since_last_project(series):\n",
    "    # Return the time in hours\n",
    "    return series.diff().dt.total_seconds() / 3600.\n",
    "\n",
    "df = ks[['category', 'launched']].sort_values('launched')\n",
    "timedeltas = df.groupby('category').transform(time_since_last_project)\n",
    "\n",
    "# Final time since last project\n",
    "timedeltas = timedeltas.fillna(timedeltas.median()).reindex(baseline_data.index)\n",
    "timedeltas.head(20)\n",
    "\n",
    "data = data.join(timedeltas.rename({'launched': 'time_since_last_project'}, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# can change numerical column using np.sqrt or np.log(colname)\n",
    "# visualize this in histogram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Count no of times target value = say  8 before current date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " def previous_attributions(series):\n",
    "        # Subtracting raw values so I don't count the current event\n",
    "        sums = series.expanding(min_periods=2).sum() - series\n",
    "        return sums"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### These all techniques we have applied for LightGBM model which is a tree network \n",
    "### Let's select and understand \"Feature selection\" for neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Univariate Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "From the scikit-learn feature selection module, feature_selection.\n",
    "SelectKBest returns the K best features given some scoring function. \n",
    "For our classification problem, the module provides three different scoring functions:  \n",
    "    χ2  \n",
    "    ANOVA F-value:- The F-value measures the linear dependency between the feature variable and the target. This means the score might underestimate the relation between a feature and the target if the relationship is nonlinear. \n",
    "    mutual information score :- nonparametric and so can capture nonlinear relationships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "\n",
    "feature_cols = data.columns.drop('outcome')\n",
    "train, valid, _ = get_data_splits(baseline_data)\n",
    "# Keep 5 features\n",
    "selector = SelectKBest(f_classif, k=5)\n",
    "\n",
    "X_new = selector.fit_transform( train[feature_cols], train['outcome'])\n",
    "X_new \n",
    "\n",
    "#we get back an array with only the selected features \"for training set\"\n",
    "\n",
    "# we need to find the columns which are dropped so that we can remove it from validation set\n",
    "we can use \".inverse_transform\" to get back an array with the shape of the original data.\n",
    "# Get back the features we've kept, zero out all other features\n",
    "selected_features = pd.DataFrame(selector.inverse_transform(X_new), \n",
    "                                 index=train.index, \n",
    "                                 columns=feature_cols)\n",
    "selected_features.head()\n",
    "\n",
    "We can find the selected columns by choosing features where the variance is non-zero.\n",
    "\n",
    "# Dropped columns have values of all 0s, so var is 0, drop them\n",
    "selected_columns = selected_features.columns[selected_features.var() != 0]\n",
    "_ = train_model(train.drop(dropped_columns, axis=1), \n",
    "                valid.drop(dropped_columns, axis=1),\n",
    "                test.drop(dropped_columns, axis=1))\n",
    "\n",
    "# Get the valid dataset with the selected features.\n",
    "# valid[selected_columns].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Univariate methods consider only one feature at a time when making a selection decision.therefore Use Lasso/Ridge regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "train, valid, _ = get_data_splits(baseline_data)\n",
    "\n",
    "X, y = train[train.columns.drop(\"outcome\")], train['outcome']\n",
    "\n",
    "# Set the regularization parameter C=1\n",
    "def select_features_l1(X, y):\n",
    "        logistic = LogisticRegression(C=0.1, penalty=\"l1\", random_state=7).fit(X, y)\n",
    "        model = SelectFromModel(logistic, prefit=True)\n",
    "\n",
    "        X_new = model.transform(X)\n",
    "\n",
    "        # Get back the kept features as a DataFrame with dropped columns as all 0s\n",
    "        selected_features = pd.DataFrame(model.inverse_transform(X_new), \n",
    "                                        index=X.index,\n",
    "                                        columns=X.columns)\n",
    "\n",
    "        # Dropped columns have values of all 0s, keep other columns \n",
    "        cols_to_keep = selected_features.columns[selected_features.var() != 0]\n",
    "\n",
    "        return cols_to_keep\n",
    "# Same as above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Label_Encoding it may happen that after train_test_split test and train has different unique values of classification(even after seeing unique values)Then we need to either drop or write custom encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# function for comparing different approaches\n",
    "def score_dataset(X_train, X_valid, y_train, y_valid):\n",
    "    model = RandomForestRegressor(n_estimators=100, random_state=0)\n",
    "    model.fit(X_train, y_train)\n",
    "    preds = model.predict(X_valid)\n",
    "    return mean_absolute_error(y_valid, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding all categorical columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All categorical columns\n",
    "object_cols = [col for col in X_train.columns if X_train[col].dtype == \"object\"]\n",
    "\n",
    "# Columns that can be safely label encoded\n",
    "good_label_cols = [col for col in object_cols if \n",
    "                   set(X_train[col]) == set(X_valid[col])]\n",
    "        \n",
    "# Problematic columns that will be dropped from the dataset\n",
    "bad_label_cols = list(set(object_cols)-set(good_label_cols))\n",
    "        \n",
    "print('Categorical columns that will be label encoded:', good_label_cols)\n",
    "print('\\nCategorical columns that will be dropped from the dataset:', bad_label_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Get list of categorical variables\n",
    "s = (X_train.dtypes == 'object')\n",
    "object_cols = list(s[s].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "\n",
    "# Make copy to avoid changing original data \n",
    "label_X_train = X_train.copy()\n",
    "label_X_valid = X_valid.copy()\n",
    "\n",
    "# Apply label encoder to each column with categorical data\n",
    "label_encoder = LabelEncoder()\n",
    "for col in object_cols:# Change it to set(good_label_cols)\n",
    "    label_X_train[col] = label_encoder.fit_transform(X_train[col])\n",
    "    label_X_valid[col] = label_encoder.transform(X_valid[col])\n",
    "\n",
    "print(\"MAE from Approach 2 (Label Encoding):\") \n",
    "# print(score_dataset(label_X_train, label_X_valid, y_train, y_valid)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "The output above shows, for each column with categorical data, the number of unique values in the column. \n",
    "For instance, the 'Street' column in the training data has two unique values: 'Grvl' and 'Pave', \n",
    "    corresponding to a gravel road and a paved road, respectively.\n",
    "\n",
    "We refer to the number of unique entries of a categorical variable as the cardinality of that categorical variable. \n",
    "For instance, the 'Street' variable has cardinality 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cardinality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get number of unique entries in each column with categorical data\n",
    "object_nunique = list(map(lambda col: X_train[col].nunique(), object_cols))\n",
    "d = dict(zip(object_cols, object_nunique))\n",
    "\n",
    "# Print number of unique entries by column, in ascending order\n",
    "sorted(d.items(), key=lambda x: x[1])\n",
    "\n",
    "For large datasets with many rows, one-hot encoding can greatly expand the size of the dataset. \n",
    "For this reason, we typically will only one-hot encode columns with relatively low cardinality. \n",
    "Then, high cardinality columns can either be dropped from the dataset, or we can use label encoding.\n",
    "\n",
    "consider a dataset with 10000 rows, and containing one categorical column with 100 unique entries.\n",
    "# How many entries are added to the dataset by replacing the column with a one-hot encoding?\n",
    "OH_entries_added = 990000(no of rows *(no of unique entries in col_c1* col_c1+so_on...)-number of entries in the original column.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columns that will be one-hot encoded\n",
    "low_cardinality_cols = [col for col in object_cols if X_train[col].nunique() < 10]\n",
    "\n",
    "# Columns that will be dropped from the dataset\n",
    "high_cardinality_cols = list(set(object_cols)-set(low_cardinality_cols))\n",
    "\n",
    "print('Categorical columns that will be one-hot encoded:', low_cardinality_cols)\n",
    "print('\\nCategorical columns that will be dropped from the dataset:', high_cardinality_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Apply one-hot encoder to each column with categorical data\n",
    "OH_encoder = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "OH_cols_train = pd.DataFrame(OH_encoder.fit_transform(X_train[object_cols]))\n",
    "OH_cols_valid = pd.DataFrame(OH_encoder.transform(X_valid[object_cols]))# only here changes happen to low cardinality\n",
    "\n",
    "# One-hot encoding removed index; put it back\n",
    "OH_cols_train.index = X_train.index\n",
    "OH_cols_valid.index = X_valid.index\n",
    "\n",
    "# Remove categorical columns (will replace with one-hot encoding)\n",
    "num_X_train = X_train.drop(object_cols, axis=1)\n",
    "num_X_valid = X_valid.drop(object_cols, axis=1)\n",
    "\n",
    "# Add one-hot encoded columns to numerical features\n",
    "OH_X_train = pd.concat([num_X_train, OH_cols_train], axis=1)\n",
    "OH_X_valid = pd.concat([num_X_valid, OH_cols_valid], axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_full.dropna(axis=0, subset=['SalePrice'], inplace=True)\n",
    "y = X_full.SalePrice\n",
    "X_full.drop(['SalePrice'], axis=1, inplace=True)\n",
    "# \"Cardinality\" means the number of unique values in a column\n",
    "\n",
    "# Select categorical columns with relatively low cardinality (convenient but arbitrary)\n",
    "low_categorical_cols = [cname for cname in X_train_full.columns if\n",
    "                    X_train_full[cname].nunique() < 10 and \n",
    "                    X_train_full[cname].dtype == \"object\"]\n",
    "\n",
    "# Select numerical columns\n",
    "numerical_cols = [cname for cname in X_train_full.columns if \n",
    "                X_train_full[cname].dtype in ['int64', 'float64']]\n",
    "\n",
    "# Keep selected columns only\n",
    "my_cols = low_categorical_cols + numerical_cols\n",
    "X_train = X_train_full[my_cols].copy()\n",
    "X_valid = X_valid_full[my_cols].copy()\n",
    "X_test = X_test_full[my_cols].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Preprocessing for numerical data\n",
    "numerical_transformer = SimpleImputer(strategy='constant')\n",
    "\n",
    "# Preprocessing for categorical data\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "# Bundle preprocessing for numerical and categorical data\n",
    "preprocessor = ColumnTransformer(transformers=[('num', numerical_transformer, numerical_cols),('cat', categorical_transformer, categorical_cols)\n",
    "    ])\n",
    "\n",
    "# Defining the model\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "model = RandomForestRegressor(n_estimators=100, criterion=\"mse\",random_state=0)\n",
    "\n",
    "# Bundle preprocessing and modeling code in a pipeline\n",
    "my_pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                              ('model', model)\n",
    "                             ])\n",
    "\n",
    "# Preprocessing of training data, fit model \n",
    "my_pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Preprocessing of validation data, get predictions\n",
    "preds = my_pipeline.predict(X_valid)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CrossValidation See  more on kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Multiply by -1 since sklearn calculates *negative* MAE\n",
    "scores = -1 * cross_val_score(my_pipeline, X, y,\n",
    "                              cv=5,\n",
    "                              scoring='neg_mean_absolute_error')\n",
    "\n",
    "print(\"MAE scores:\\n\", scores)\n",
    "print(\"Average MAE score (across experiments):\")\n",
    "print(scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_score(n_estimators):\n",
    "    \"\"\"Return the average MAE over 3 CV folds of random forest model.\n",
    "    \n",
    "    Keyword argument:\n",
    "    n_estimators -- the number of trees in the forest\n",
    "    \"\"\"\n",
    "    my_pipeline = Pipeline(steps=[('preprocessor', SimpleImputer()),('model', RandomForestRegressor(n_estimators, random_state=0))])\n",
    "    scores = -1 * cross_val_score(my_pipeline, X, y,cv= 3,scoring='neg_mean_absolute_error')\n",
    "    return scores.mean()\n",
    "    pass\n",
    "List = [50,100,150,200,250,300,350,400]\n",
    "results = dict(zip(List, (get_score(x) for x in List)))\n",
    "\n",
    "plt.plot(list(results.keys()), list(results.values()))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Boosting/Ensembling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encode the data (to shorten the code, we use pandas)\n",
    "X_train = pd.get_dummies(X_train)\n",
    "X_valid = pd.get_dummies(X_valid)\n",
    "X_test = pd.get_dummies(X_test)\n",
    "X_train, X_valid = X_train.align(X_valid, join='left', axis=1)\n",
    "X_train, X_test = X_train.align(X_test, join='left', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "from xgboost import XGBRegressor\n",
    "my_model_2 = XGBRegressor(n_estimators = 500 , learning_rate = 0.01,n_jobs =4)\n",
    "\n",
    "# Fit the model\n",
    "my_model_2.fit(X_train, y_train, \n",
    "             early_stopping_rounds=5, \n",
    "             eval_set=[(X_valid, y_valid)], \n",
    "             verbose=False)\n",
    "\n",
    "# Get predictions\n",
    "predictions_2 = my_model_2.predict(X_valid)\n",
    "\n",
    "# Calculate MAE\n",
    "mae_2 = mean_absolute_error(predictions_2 , y_valid)\n",
    "\n",
    "# Uncomment to print MAE\n",
    "print(\"Mean Absolute Error:\" , mae_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "In Xgboost we have XGB Classifier() which has lots of params which one to choose is difficult choice \n",
    "which is solved by randomsearch parameter which will go in permutation and combination of each value of this learning rate  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = xgboost.classifier()\n",
    "params = {\n",
    "            learning_rate: [0.01 , 0.10, 0.15 , 0.20 , 0.25 ]\n",
    "            n_estimators: 100 #f the size of your data is high, 1000 is if it is medium-low\n",
    "            max_depth: [3, 4 , 5 , 6 , 9 , 10 , 12 ]\n",
    "            subsample: [0.8,0.9...]\n",
    "            colsample_bytree: [ 0.1 , 0.5 , 1.0 ,  0.7 ]\n",
    "            gamma: [ 0 , 1 , 5 ]\n",
    "         }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Leakage\n",
    "\n",
    "##### In other words, leakage causes a model to look accurate until you start making decisions with the model, and then the model becomes very inaccurate.\n",
    "\n",
    "##### There are two main types of leakage: target leakage and train-test contamination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For target leakage- Model gives high scores at the time of validation but will give inaccurate results in real-time data. \n",
    "To prevent this type of data leakage, \n",
    "any variable updated (or created) after the target value is realized should be excluded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-test contamination -\n",
    " Occurs when you aren't careful to distinguish training data from validation data.\n",
    "    Your model may get good validation scores, giving you great confidence in it, \n",
    "    but perform poorly when you deploy it to make decisions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the data\n",
    "data = pd.read_csv('../input/aer-credit-card-data/AER_credit_card_data.csv', \n",
    "                   true_values = ['yes'], false_values = ['no'])\n",
    "\n",
    "# Select target\n",
    "y = data.card\n",
    "\n",
    "# Select predictors\n",
    "X = data.drop(['card'], axis=1)\n",
    "# Check some data comparisons on some columns which are not clear\n",
    "\n",
    "# card: 1 if credit card application accepted, 0 if not\n",
    "# reports: Number of major derogatory reports\n",
    "# age: Age n years plus twelfths of a year\n",
    "# income: Yearly income (divided by 10,000)\n",
    "# share: Ratio of monthly credit card expenditure to yearly income\n",
    "# expenditure: Average monthly credit card expenditure\n",
    "# owner: 1 if owns home, 0 if rents\n",
    "# selfempl: 1 if self-employed, 0 if not\n",
    "# dependents: 1 + number of dependents\n",
    "# months: Months living at current address\n",
    "# majorcards: Number of major credit cards held\n",
    "# active: Number of active credit accounts\n",
    "# A few variables look suspicious. For example, does expenditure mean expenditure on this card or on cards used before appying?\n",
    "\n",
    "\n",
    "expenditures_cardholders = X.expenditure[y]\n",
    "expenditures_noncardholders = X.expenditure[~y]\n",
    "\n",
    "print('Fraction of those who did not receive a card and had no expenditures: %.2f' \\\n",
    "      %((expenditures_noncardholders == 0).mean()))\n",
    "print('Fraction of those who received a card and had no expenditures: %.2f' \\\n",
    "      %(( expenditures_cardholders == 0).mean()))\n",
    "\n",
    "# Drop leaky predictors from dataset\n",
    "potential_leaks = ['expenditure', 'share', 'active', 'majorcards']\n",
    "X2 = X.drop(potential_leaks, axis=1)\n",
    "\n",
    "# Evaluate the model with leaky predictors removed\n",
    "my_pipeline = make_pipeline(RandomForestClassifier(n_estimators=100))\n",
    "cv_scores = cross_val_score(my_pipeline, X2, y, \n",
    "                            cv=5,\n",
    "                            scoring='accuracy')\n",
    "\n",
    "print(\"Cross-val accuracy: %f\" % cv_scores.mean())\n",
    "\n",
    "# This accuracy is quite a bit lower, which might be disappointing. \n",
    "# However, we can expect it to be right about 80% of the time when used on new applications, \n",
    "# whereas the leaky model would likely do much worse than that (in spite of its higher apparent score in cross-validation).\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
